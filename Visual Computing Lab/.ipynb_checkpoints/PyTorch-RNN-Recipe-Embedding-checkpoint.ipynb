{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from io import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procData(path):\n",
    "    file = open(path)\n",
    "    \n",
    "    data = \"\"\n",
    "    count = 1\n",
    "    for line in file.readlines():\n",
    "        data = data + (line).lower()\n",
    "        count = count + 1\n",
    "        if count % 200000 == 0:\n",
    "            break\n",
    "\n",
    "    data = re.sub(\"\\\\t\",\"\", data)\n",
    "    data = re.sub(\"\\\\n\",\"\", data)\n",
    "    data = re.sub(\"  \", \" \", data)\n",
    "    data = data.split(\" . \")\n",
    "\n",
    "    holder = []\n",
    "    for d in data:\n",
    "        if d != \"\":\n",
    "            holder.append(d)   \n",
    "    data = holder\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i] + \" . \"\n",
    "        \n",
    "    o1 = set(''.join(data).split(\" \"))\n",
    "        \n",
    "    for j in range(len(data)):\n",
    "        data[j] = data[j].split(\" \")\n",
    "\n",
    "        holder = []\n",
    "        for i in range(len(data[j])):\n",
    "            if data[j][i] != \"\":\n",
    "                holder.append(data[j][i])\n",
    "        data[j] = holder  \n",
    "    \n",
    "    return data, o1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"recipe_tokenized_instructions_.txt\"\n",
    "data, vocab = procData(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quatragrams = []\n",
    "for j in range(len(data)):\n",
    "    padded = data[j][:]\n",
    "    for k in range(3):\n",
    "        padded.insert(0,\"\")\n",
    "    for i in range(len(padded)-3):\n",
    "        quatragrams.append([[padded[i],padded[i+1],padded[i+2]], padded[i+3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "EMBEDDING_DIM = 50\n",
    "word_indices = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "    def getEmb(self):\n",
    "        return self.embeddings\n",
    "    \n",
    "    def getProb(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1, -1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1000):\n",
    "    #if (epoch % 1 == 0):\n",
    "        #print(\"Epoch: \", epoch)\n",
    "    total_loss = 0\n",
    "    for context, target in quatragrams:\n",
    "        \n",
    "        context_idxs = torch.tensor([word_indices[w] for w in context], dtype=torch.long)\n",
    "        model.zero_grad()\n",
    "        log_probs = model(context_idxs)\n",
    "        loss = loss_function(log_probs, torch.tensor([word_indices[target]], dtype=torch.long))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    losses.append(round(total_loss, 3))\n",
    "#print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = [model.embeddings(torch.tensor(word_indices[w])) for w in vocab]\n",
    "#print(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0.0\n",
    "for i in word_indices:\n",
    "    em = model.embeddings(torch.tensor([word_indices[i]]))\n",
    "    t = model.getEmb()\n",
    "    normalized_embedding = t.weight/((t.weight**2).sum(0)**0.5).expand_as(t.weight)\n",
    "    dist, ind = torch.topk(torch.mv(normalized_embedding,em[0]),5)\n",
    "    \n",
    "    estimation = int(ind[0])\n",
    "    target = word_indices[i]\n",
    "    \n",
    "    if (estimation == target):\n",
    "        count += 1.0\n",
    "\n",
    "print(count/len(word_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emFile = open(\"Em.pickle\",\"wb\")\n",
    "pickle.dump(emb, emFile)\n",
    "emFile.close()\n",
    "\n",
    "wiFile = open(\"WI.pickle\",\"wb\")\n",
    "pickle.dump(word_indices, wiFile)\n",
    "wiFile.close()\n",
    "\n",
    "gramFile = open(\"QG.pickle\",\"wb\")\n",
    "pickle.dump(quatragrams, gramFile)\n",
    "gramFile.close()\n",
    "\n",
    "vocabFile = open(\"Vocab.pickle\",\"wb\")\n",
    "pickle.dump(vocab, vocabFile)\n",
    "vocabFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
